## How to connect to the **UDAL Warehouse**

To connect to the UDAL data warehouse, you need to select *Azure Synapse Analytics* as datasource.

When prompted, fill the Server and Database fields using teh folowing:
> **Server:** *udalsyndataprod.sql.azuresynapse.net*
> **Database:**  *UDAL_Warehouse*

Use your UDAL username and password to finalise the connection.


* Tableau
    Click on More and choose *Azure Synapse Analytics*
    ![alt text](docs/pics/image.png)
    

* Excel
    In the *Data* tab, "Get Data" > "From Azure" > "From Azure Synapse Analytics"
    ![alt text](docs/pics/image-1.png)
    

* Power BI
    Click on "Get data from other source --->" and then select *Azure Synapse Analytics SQL* from the list
    ![alt text](docs/pics/image-2.png)


* DataBricks
    Follow the instructions here [Link](https://nhs.sharepoint.com/sites/msteams_793886-SouthEast/Shared%20Documents/South%20East/L&D/Databricks/Introduction%20to%20Databricks.pptx?web=1).
    The example below for the APCS_Core_Daily SUS table.
    
    > ```
    > lakeName = "udalstdatacuratedprod.dfs.core.windows.net"      # lake name
    >
    > containerName = "restricted"                                 # high level folder
    >
    > fileLoc = "/patientlevel/MESH/APC/APCS_Core_Daily/"          # folder where dataset is housed in azure
    >
    > path = "abfss://"+containerName+"@"+lakeName+fileLoc
    >
    > df = spark.read.option("header", "true") \
    >                .option("recursiveFileLookup", "True") \
    >                .parquet(path)
    >```
    
    
* R
    Use the script below
    
    > ```
    > # Establish UDAL connections ----------------------------------------------
    > # insert your UDAL user ID
    > library(svDialogs)
    > uid <- dlgInput("Enter udal ID", Sys.info()["user"])$res
    > 
    > # establish connection to UDAL
    > con_udal <- DBI::dbConnect(drv = odbc::odbc(),
    >                        driver = "ODBC Driver 17 for SQL Server",
    >                        server = "udalsyndataprod.sql.azuresynapse.net",
    >                        database = "UDAL_Warehouse",
    >                        UID = uid,
    >                        authentication = "ActiveDirectoryInteractive")
    > 
    > # import data ----------------------------------------------------------
    > # in this example we use a query to define the data to pull
    > string_sql <- readr::read_file("data/udal queries/sql_query_udal.txt")
    > df_data <- DBI::dbGetQuery(conn = con_udal, statement = string_sql)
    > 
    > # Close connection --------------------------------------------------------
    > DBI::dbDisconnect(con_udal)
    > ```
    
**NOTE: R will open a window _in the background_ to enter the UDAL credentials, without any notification!!!**




## How to connect to the **SE D&A LakeMart**

### How to _read_ an existing file from the LakeMart

To connect to the SE D&A LakeMart, you need to select *Azure Data Lake Storage Gen2* as datasource.

When prompted, enter the endpoint URL (ex. *https://udalstdataanalysisprod.dfs.core.windows.net/*) to browse through the files available to you, or if you have a direct File URL (ex. *https://udalstdataanalysisprod.dfs.core.windows.net/analytics-projects/SEAnalytics/Alberto/Test.csv*)

**NOTE: to obtain the file URL, browse to the file in the _Microsoft Azure Storage Explorer_, right-click on the file, click on "Copy URL" and then "With DFS Endpoint"**
![alt text](docs/pics/image-3.png)

**NOTE: Using the endpoint URL does not work for me, however I have no issues using the direct file URL**

Use your UDAL username and password to finalise the connection.


* Tableau
    Click on More and choose *Azure Data Lake Storage Gen2*
    ![alt text](docs/pics/image-4.png)

* Excel
    In the *Data* tab, "Get Data" > "From Azure" > "From Azure Data Lake Storage Gen2"
    ![alt text](docs/pics/image-5.png)

* Power BI
    Click on "Get data from other source --->" and then select *Azure Data Lake Storage Gen2* from the list
    ![alt text](docs/pics/image-6.png)


* DataBricks
    Use the example script below to read an existing _csv_ file.
    
    > ```
    > ### use the direct URL to the file you want to read instead of the exapmle URL
    > my_df = spark.read.csv(
    >     "abfss://analytics-projects@udalstdataanalysisprod.dfs.core.windows.net/SEAnalytics/Alberto/Test.csv"     
    >     , header="true")
    >
    > # to visualise the table stored in my_df dataframe
    > display(my_df)
    >```

* R
    Use the script below to read an existing *coalesced* _csv_ file
    
    > ```
    > # Load libraries
    > library(AzureStor)
    > library(AzureKeyVault)
    > ## ---- Set Up 
    > ## this sets the secrets needed to connect to Azure (based on East of England advice December 24)
    > ## this file (along with config.py) is specifically referred to in gitignore
    > 
    > key_vault_url = "https://udal-kv-stsas-prod.vault.azure.net/" # keep this as is
    > 
    > team_secret = "alp-analytics-projects-SEAnalytics-w"  
    > # the -w at the end means it's picking up the secret with WRITE permissions, if you do -r this is READ ONLY so can limit you.
    > 
    > lake_url = "https://udalstdataanalysisprod.dfs.core.windows.net" # keep this as is
    > 
    > storage_container_name = "analytics-projects" # keep this as is
    > 
    > 
    > # #set file name to be brought into R (i.e. the file that's in the storage explorer)
    > # # this file gets updated by a Databricks process with timestamped reference versions stored separately
    > 
    > filenamecsv = "SEAnalytics/Alberto/Test.csv"
    >     
    > 
    > # Ingest the data
    > # ------------------------------------------------------------------------------
    > 
    > # load secrets -----------------------------------------------------------------
    > ## see separate configR.R (excluded using gitignore)
    > 
    > # Get Shared Access Signature auth for data
    > # Opens an interactive Azure authentication flow in your browser
    > vault <- key_vault(key_vault_url)
    > 
    > # Read from specific lake area
    > # Gets the SAS for the relevant data - r = read access, w = write access
    > sas_token <- vault$secrets$get(team_secret)
    > 
    > # Set connection settings
    > storage_account <- storage_endpoint(lake_url, 
    >                                     sas = sas_token$value)
    > 
    > container <- storage_container(storage_account, storage_container_name)
    > ```


<!--- NOT DONE YET --->
<!---              --->
### How to _write_ in the LakeMart

* DataBricks
    To save a dataframe/table as a csv file, use the script below
    
    > ```
    > # Create a coalesced output csv file
    > my_df.coalesce(1).write.mode('overwrite').option("header", "true").csv(
    >     "abfss://analytics-projects@udalstdataanalysisprod.dfs.core.windows.net/SEAnalytics/Alberto/"
    > )
    > 
    > # Step 1: List the files in the folder where the coalesced output is saved
    > files = dbutils.fs.ls("abfss://analytics-projects@udalstdataanalysisprod.dfs.core.windows.net/SEAnalytics/Alberto/")
    > 
    > # Step 2: Find the single file (i.e. the one that has been produced)
    > # Filter for part files (e.g., part-00000...)
    > part_files = [f for f in files if f.path.startswith('abfss://analytics-projects@udalstdataanalysisprod.dfs.core.windows.net/SEAnalytics/Alberto/part-')]
    > 
    > # Step 3: Sort part files by modification time to get the latest one (there should only be one as it's coalesced)
    > latest_file = max(part_files, key=lambda f: f.modificationTime)
    > 
    > # Step 4: Rename the csv file with a dynamic timestamp
    > timestamp = datetime.now().strftime("%Y-%m-%d")
    > destination_file = f"abfss://analytics-projects@udalstdataanalysisprod.dfs.core.windows.net/SEAnalytics/Alberto/Test.csv"
    > 
    > # Step 5: Move the file to the final destination
    > dbutils.fs.mv(latest_file.path, destination_file)
    > 
    > # Step 6: Delete files created as by-product
    > underscore_files = [f for f in files if f.path.startswith('abfss://analytics-projects@udalstdataanalysisprod.dfs.core.windows.net/SEAnalytics/Alberto/_')]
    > for file in underscore_files:
    >     dbutils.fs.rm(file.path)
    >```